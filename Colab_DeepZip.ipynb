{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oONeCET9yAo0"
      },
      "outputs": [],
      "source": [
        "#@title Import Libraries\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.models import Sequential\n",
        "from keras.models import model_from_json\n",
        "from keras.models import load_model\n",
        "# from keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.layers import Dense, Bidirectional, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Dropout\n",
        "from keras.layers import LSTM, Flatten, Conv1D, LocallyConnected1D, GRU, Layer, MultiHeadAttention, LayerNormalization\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger\n",
        "from keras import backend as K\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import contextlib\n",
        "import struct\n",
        "import tempfile\n",
        "import shutil\n",
        "from math import sqrt\n",
        "from tqdm.notebook import tqdm\n",
        "from tensorflow.keras.utils import Progbar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rk1b0zEWkJAZ"
      },
      "outputs": [],
      "source": [
        "#@title GPU Initialization\n",
        "#Execute only if using GPU\n",
        "\n",
        "# strategy = tf.distribute.MirroredStrategy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "drkx7OFUEo-9"
      },
      "outputs": [],
      "source": [
        "#@title TPU Initialization\n",
        "# Execute only if using TPU\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKZyw5N_NbsK",
        "outputId": "9b4c5b5b-f840-41e3-9a85-4a7f9840dc2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsXf-eJvijCH"
      },
      "outputs": [],
      "source": [
        "base = 'HP1'\n",
        "\n",
        "with open('/content/drive/MyDrive/DeepZip/' + 'data/files_to_be_compressed/' + base + '.txt', 'rb') as fp:\n",
        "    data = fp.read()\n",
        "\n",
        "clean_data = \"\"\n",
        "for character in data:\n",
        "    if character < 128:\n",
        "        clean_data += chr(character)\n",
        "\n",
        "with open('/content/drive/MyDrive/DeepZip/' + 'data/files_to_be_compressed/' + base + '_clean.txt', 'w') as fp:\n",
        "    fp.write(clean_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjr7P6dHpq3R",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Data Preparation, Choosing Input File and Model\n",
        "#data preparation - parse_new.py\n",
        "\n",
        "base_path = '/content/drive/MyDrive/DeepZip/'\n",
        "base_name = 'HP1_clean'\n",
        "model_name = 'TransformerLarge_128'\n",
        "\n",
        "input_file_path = base_path + 'data/files_to_be_compressed/' + base_name + '.txt'\n",
        "param_file_path = base_path + 'data/processed_files/' + base_name + '.param.json'\n",
        "output_file_path = base_path + 'data/processed_files/' + base_name + '.npy'\n",
        "\n",
        "with open(input_file_path, 'rb') as fp:\n",
        "    data = fp.read()\n",
        "\n",
        "print(len(data))\n",
        "vals = list(set(data))\n",
        "\n",
        "print(len(vals))\n",
        "\n",
        "char2id_dict = {c: i for (i,c) in enumerate(vals)}\n",
        "id2char_dict = {i: c for (i,c) in enumerate(vals)}\n",
        "\n",
        "params = {'char2id_dict':char2id_dict, 'id2char_dict':id2char_dict}\n",
        "with open(param_file_path, 'w') as f:\n",
        "    json.dump(params, f, indent=4)\n",
        "\n",
        "print(char2id_dict)\n",
        "print(id2char_dict)\n",
        "\n",
        "out = [char2id_dict[c] for c in data]\n",
        "integer_encoded = np.array(out)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "print(integer_encoded[:10])\n",
        "print(data[:10])\n",
        "\n",
        "np.save(output_file_path, integer_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp1l2OsntYh3",
        "outputId": "cc67c7ce-253c-4355-8d05-41670c300841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(77, 128)\n"
          ]
        }
      ],
      "source": [
        "conversion_matrix = [[0]*128 for i in range(len(vals))]\n",
        "\n",
        "for i in range(len(vals)):\n",
        "    conversion_matrix[i][vals[i]] = 1\n",
        "\n",
        "conversion_matrix = np.array(conversion_matrix).astype('uint8')\n",
        "\n",
        "print(conversion_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eZdGm7A9ylEY"
      },
      "outputs": [],
      "source": [
        "#@title Arithmetic Encoder\n",
        "\n",
        "class ArithmeticCoderBase(object):\n",
        "\t\n",
        "\tdef __init__(self, statesize):\n",
        "\t\tself.STATE_SIZE = statesize\n",
        "\t\tself.MAX_RANGE = 1 << self.STATE_SIZE\n",
        "\t\tself.MIN_RANGE = (self.MAX_RANGE >> 2) + 2\n",
        "\t\tself.MAX_TOTAL = self.MIN_RANGE\n",
        "\t\tself.MASK = self.MAX_RANGE - 1\n",
        "\t\tself.TOP_MASK = self.MAX_RANGE >> 1\n",
        "\t\tself.SECOND_MASK = self.TOP_MASK >> 1\n",
        "\t\tself.low = 0\n",
        "\t\tself.high = self.MASK\n",
        "\t\n",
        "\tdef update(self,  cumul, symbol):\n",
        "\t\tlow = self.low\n",
        "\t\thigh = self.high\n",
        "\n",
        "\t\trange = high - low + 1\n",
        "\n",
        "\t\ttotal = np.asscalar(cumul[-1])\n",
        "\t\tsymlow = np.asscalar(cumul[symbol])\n",
        "\t\tsymhigh = np.asscalar(cumul[symbol+1])\n",
        "\n",
        "\t\tnewlow  = low + symlow  * range // total\n",
        "\t\tnewhigh = low + symhigh * range // total - 1\n",
        "\t\tself.low = newlow\n",
        "\t\tself.high = newhigh\n",
        "\t\t\n",
        "\t\twhile ((self.low ^ self.high) & self.TOP_MASK) == 0:\n",
        "\t\t\tself.shift()\n",
        "\t\t\tself.low = (self.low << 1) & self.MASK\n",
        "\t\t\tself.high = ((self.high << 1) & self.MASK) | 1\n",
        "\t\t\n",
        "\t\twhile (self.low & ~self.high & self.SECOND_MASK) != 0:\n",
        "\t\t\tself.underflow()\n",
        "\t\t\tself.low = (self.low << 1) & (self.MASK >> 1)\n",
        "\t\t\tself.high = ((self.high << 1) & (self.MASK >> 1)) | self.TOP_MASK | 1\n",
        "\t\n",
        "\tdef shift(self):\n",
        "\t\traise NotImplementedError()\n",
        "\t\n",
        "\tdef underflow(self):\n",
        "\t\traise NotImplementedError()\n",
        "\n",
        "class ArithmeticEncoder(ArithmeticCoderBase):\n",
        "\t\n",
        "\tdef __init__(self, statesize, bitout):\n",
        "\t\tsuper(ArithmeticEncoder, self).__init__(statesize)\n",
        "\t\tself.output = bitout\n",
        "\t\tself.num_underflow = 0\n",
        "\t\n",
        "\tdef write(self, cumul, symbol):\n",
        "\t\tself.update(cumul, symbol)\n",
        "\t\n",
        "\tdef finish(self):\n",
        "\t\tself.output.write(1)\n",
        "\t\n",
        "\tdef shift(self):\n",
        "\t\tbit = self.low >> (self.STATE_SIZE - 1)\n",
        "\t\tself.output.write(bit)\n",
        "\t\tfor _ in range(self.num_underflow):\n",
        "\t\t\tself.output.write(bit ^ 1)\n",
        "\t\tself.num_underflow = 0\n",
        "\t\n",
        "\tdef underflow(self):\n",
        "\t\tself.num_underflow += 1\n",
        "\n",
        "class ArithmeticDecoder(ArithmeticCoderBase):\n",
        "\t\n",
        "\tdef __init__(self, statesize, bitin):\n",
        "\t\tsuper(ArithmeticDecoder, self).__init__(statesize)\n",
        "\t\tself.input = bitin\n",
        "\t\tself.code = 0\n",
        "\t\tfor _ in range(self.STATE_SIZE):\n",
        "\t\t\tself.code = self.code << 1 | self.read_code_bit()\n",
        "\t\n",
        "\tdef read(self, cumul, alphabet_size):\n",
        "\t\ttotal = np.asscalar(cumul[-1])\n",
        "\t\trange = self.high - self.low + 1\n",
        "\t\toffset = self.code - self.low\n",
        "\t\tvalue = ((offset + 1) * total - 1) // range\n",
        "\t\tstart = 0\n",
        "\t\tend = alphabet_size\n",
        "\t\twhile end - start > 1:\n",
        "\t\t\tmiddle = (start + end) >> 1\n",
        "\t\t\tif cumul[middle] > value:\n",
        "\t\t\t\tend = middle\n",
        "\t\t\telse:\n",
        "\t\t\t\tstart = middle\n",
        "\t\t\n",
        "\t\tsymbol = start\n",
        "\t\tself.update(cumul, symbol)\n",
        "\t\treturn symbol\n",
        "\t\n",
        "\tdef shift(self):\n",
        "\t\tself.code = ((self.code << 1) & self.MASK) | self.read_code_bit()\n",
        "\t\n",
        "\tdef underflow(self):\n",
        "\t\tself.code = (self.code & self.TOP_MASK) | ((self.code << 1) & (self.MASK >> 1)) | self.read_code_bit()\n",
        "\t\n",
        "\tdef read_code_bit(self):\n",
        "\t\ttemp = self.input.read()\n",
        "\t\tif temp == -1:\n",
        "\t\t\ttemp = 0\n",
        "\t\treturn temp\n",
        "\n",
        "\n",
        "class BitInputStream(object):\n",
        "\t\n",
        "\tdef __init__(self, inp):\n",
        "\t\tself.input = inp\n",
        "\t\tself.currentbyte = 0\n",
        "\t\tself.numbitsremaining = 0\n",
        "\t\n",
        "\tdef read(self):\n",
        "\t\tif self.currentbyte == -1:\n",
        "\t\t\treturn -1\n",
        "\t\tif self.numbitsremaining == 0:\n",
        "\t\t\ttemp = self.input.read(1)\n",
        "\t\t\tif len(temp) == 0:\n",
        "\t\t\t\tself.currentbyte = -1\n",
        "\t\t\t\treturn -1\n",
        "\t\t\tself.currentbyte = temp[0]\n",
        "\t\t\tself.numbitsremaining = 8\n",
        "\t\tassert self.numbitsremaining > 0\n",
        "\t\tself.numbitsremaining -= 1\n",
        "\t\treturn (self.currentbyte >> self.numbitsremaining) & 1\n",
        "\t\n",
        "\tdef read_no_eof(self):\n",
        "\t\tresult = self.read()\n",
        "\t\tif result != -1:\n",
        "\t\t\treturn result\n",
        "\t\telse:\n",
        "\t\t\traise EOFError()\n",
        "\t\n",
        "\tdef close(self):\n",
        "\t\tself.input.close()\n",
        "\t\tself.currentbyte = -1\n",
        "\t\tself.numbitsremaining = 0\n",
        "\n",
        "\n",
        "class BitOutputStream(object):\n",
        "\t\n",
        "\tdef __init__(self, out):\n",
        "\t\tself.output = out\n",
        "\t\tself.currentbyte = 0\n",
        "\t\tself.numbitsfilled = 0\n",
        "\t\n",
        "\tdef write(self, b):\n",
        "\t\tif b not in (0, 1):\n",
        "\t\t\traise ValueError(\"Argument must be 0 or 1\")\n",
        "\t\tself.currentbyte = (self.currentbyte << 1) | b\n",
        "\t\tself.numbitsfilled += 1\n",
        "\t\tif self.numbitsfilled == 8:\n",
        "\t\t\ttowrite = bytes((self.currentbyte,))\n",
        "\t\t\tself.output.write(towrite)\n",
        "\t\t\tself.currentbyte = 0\n",
        "\t\t\tself.numbitsfilled = 0\n",
        "\t\n",
        "\tdef close(self):\n",
        "\t\twhile self.numbitsfilled != 0:\n",
        "\t\t\tself.write(0)\n",
        "\t\tself.output.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lXh9e__v2z7b"
      },
      "outputs": [],
      "source": [
        "#@title Models\n",
        "\n",
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"), Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class Models():\n",
        "\n",
        "    def biGRU(bs,time_steps,alphabet_size):\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(alphabet_size, 32))\n",
        "        model.add(Bidirectional(GRU(32, stateful=False, return_sequences=True)))\n",
        "        model.add(Bidirectional(GRU(32, stateful=False, return_sequences=False)))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(alphabet_size, activation='softmax'))\n",
        "        return model\n",
        "\n",
        "    # def biGRU_big(bs,time_steps,alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(Bidirectional(GRU(128, stateful=False, return_sequences=True)))\n",
        "    #     model.add(Bidirectional(GRU(128, stateful=False, return_sequences=False)))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def biGRU_16bit(bs,time_steps,alphabet_size):\n",
        "    #     K.set_floatx('float16')\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(Bidirectional(GRU(32, stateful=False, return_sequences=True)))\n",
        "    #     model.add(Bidirectional(GRU(32, stateful=False, return_sequences=False)))\n",
        "    #     model.add(Dense(64, activation='relu'))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def biLSTM(bs,time_steps,alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(Bidirectional(LSTM(32, stateful=False, return_sequences=True)))\n",
        "    #     model.add(Bidirectional(LSTM(32, stateful=False, return_sequences=False)))\n",
        "    #     model.add(Dense(64, activation='relu'))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def biLSTM_16bit(bs,time_steps,alphabet_size):\n",
        "    #     K.set_floatx('float16')\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(Bidirectional(LSTM(32, stateful=False, return_sequences=True)))\n",
        "    #     model.add(Bidirectional(LSTM(32, stateful=False, return_sequences=False)))\n",
        "    #     model.add(Dense(64, activation='relu'))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def LSTM_multi(bs,time_steps,alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(64, activation='relu'))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def LSTM_multi_big(bs,time_steps,alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 64, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(LSTM(64, stateful=False, return_sequences=True))\n",
        "    #     model.add(LSTM(64, stateful=False, return_sequences=True))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def LSTM_multi_bn(bs,time_steps,alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(BatchNormalization())\n",
        "    #     model.add(Dense(64, activation='relu'))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def LSTM_multi_16bit(bs,time_steps,alphabet_size):\n",
        "    #     K.set_floatx('float16')\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(64, activation='relu'))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def LSTM_multi_selu(bs,time_steps,alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(64, activation=keras.activations.selu, kernel_initializer=init))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def LSTM_multi_selu_16bit(bs,time_steps,alphabet_size):\n",
        "    #     K.set_floatx('float16')\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(LSTM(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(Flatten())\n",
        "    #     init = keras.initializers.lecun_uniform(seed=0)\n",
        "    #     model.add(Dense(64, activation=keras.activations.selu, kernel_initializer=init))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def GRU_multi(bs,time_steps,alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(GRU(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(GRU(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(64, activation='relu'))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def GRU_multi_big(bs,time_steps,alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(GRU(128, stateful=False, return_sequences=True))\n",
        "    #     model.add(GRU(128, stateful=False, return_sequences=True))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def GRU_multi_16bit(bs,time_steps,alphabet_size):\n",
        "    #     K.set_floatx('float16')\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(GRU(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(GRU(32, stateful=False, return_sequences=True))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(64, activation='relu'))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def FC_4layer_16bit(bs,time_steps, alphabet_size):\n",
        "    #     K.set_floatx('float16')\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 5, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def FC_4layer(bs,time_steps, alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 5, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def FC_4layer_big(bs,time_steps, alphabet_size):\n",
        "    #     model = Sequential()\n",
        "    #     model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(Flatten())\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(128, activation=ELU(1.0)))\n",
        "    #     model.add(Dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    # def FC_16bit(bs,time_steps,alphabet_size):\n",
        "    #     k.set_floatx('float16')\n",
        "    #     model = Sequential()\n",
        "    #     init = keras.initializers.lecun_uniform(seed=0)\n",
        "    #     model.add(embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "    #     model.add(flatten())\n",
        "    #     model.add(dense(1024, activation='relu', kernel_initializer=init))\n",
        "    #     model.add(dense(64, activation='relu', kernel_initializer=init))\n",
        "    #     model.add(dense(alphabet_size, activation='softmax'))\n",
        "    #     return model\n",
        "\n",
        "    def FC(bs,time_steps,alphabet_size):\n",
        "        model = Sequential()\n",
        "        init = keras.initializers.lecun_uniform(seed=0)\n",
        "        model.add(Embedding(alphabet_size, 32, batch_input_shape=(bs, time_steps)))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(1024, activation='relu', kernel_initializer=init))\n",
        "        model.add(Dense(64, activation='relu', kernel_initializer=init))\n",
        "        model.add(Dense(alphabet_size, activation='softmax'))\n",
        "        return model\n",
        "\n",
        "    def Transformer(bs, time_steps, alphabet_size):\n",
        "        model = Sequential()\n",
        "        model.add(TokenAndPositionEmbedding(time_steps, alphabet_size, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        model.add(Dropout(0.1))\n",
        "        # model.add(Dense(64, activation=\"relu\"))\n",
        "        # model.add(Dropout(0.1))\n",
        "        model.add(Dense(alphabet_size, activation=\"softmax\"))\n",
        "        return model\n",
        "\n",
        "    def Trans2(bs, time_steps, alphabet_size):\n",
        "        model = Sequential()\n",
        "        model.add(TokenAndPositionEmbedding(time_steps, alphabet_size, 1024))\n",
        "        model.add(TransformerBlock(1024, 2, 1024))\n",
        "        # model.add(TransformerBlock(256, 2, 256))\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        model.add(Dropout(0.1))\n",
        "        # model.add(Dense(128, activation=\"relu\"))\n",
        "        # model.add(Dropout(0.1))\n",
        "        model.add(Dense(alphabet_size, activation=\"softmax\"))\n",
        "        return model\n",
        "\n",
        "    def Transformer3(bs, time_steps, alphabet_size):\n",
        "        model = Sequential()\n",
        "        model.add(TokenAndPositionEmbedding(time_steps, alphabet_size, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        model.add(Dropout(0.1))\n",
        "        #model.add(Dense(64, activation=\"relu\"))\n",
        "        #model.add(Dropout(0.1))\n",
        "        model.add(Dense(alphabet_size, activation=\"softmax\"))\n",
        "        return model\n",
        "\n",
        "    def Transformer1(bs, time_steps, alphabet_size):\n",
        "        model = Sequential()\n",
        "        model.add(TokenAndPositionEmbedding(time_steps, alphabet_size, 32))\n",
        "        model.add(TransformerBlock(32, 2, 32))\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        model.add(Dropout(0.1))\n",
        "        #model.add(Dense(64, activation=\"relu\"))\n",
        "        #model.add(Dropout(0.1))\n",
        "        model.add(Dense(alphabet_size, activation=\"softmax\"))\n",
        "        return model\n",
        "\n",
        "    def TransformerLarge_128(bs, time_steps, alphabet_size):\n",
        "        model = Sequential()\n",
        "        model.add(TokenAndPositionEmbedding(time_steps, alphabet_size, 1024))\n",
        "        model.add(TransformerBlock(1024, 2, 1024))\n",
        "        model.add(TransformerBlock(1024, 2, 1024))\n",
        "        model.add(GlobalAveragePooling1D())\n",
        "        model.add(Dropout(0.1))\n",
        "        # model.add(Dense(64, activation=\"relu\"))\n",
        "        # model.add(Dropout(0.1))\n",
        "        model.add(Dense(alphabet_size, activation=\"softmax\"))\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "J0glE3RijLXX"
      },
      "outputs": [],
      "source": [
        "#@title Create Directories and Paths\n",
        "#script \n",
        "\n",
        "model_dir = base_path + 'data/trained_models/'\n",
        "compressed_dir = base_path + 'data/compressed/'\n",
        "data_dir = base_path + 'data/processed_files/'\n",
        "logs_dir = base_path + 'data/logs_data/'\n",
        "original_dir = base_path + 'data/files_to_be_compressed/'\n",
        "\n",
        "try:\n",
        "    os.mkdir(model_dir + base_name)\n",
        "except OSError as error:\n",
        "    print(error)\n",
        "\n",
        "model_file = model_dir + base_name + '/' + model_name + '.hdf5'\n",
        "log_file = logs_dir + base_name + '/' + model_name + '.log.csv'\n",
        "output_dir = compressed_dir + base_name\n",
        "recon_file_name = output_dir + '/' + model_name + '.reconstructed.txt'\n",
        "params_file = data_dir + base_name + '.param.json'\n",
        "print(params_file)\n",
        "output_prefix = output_dir + '/' + model_name + '.compressed'\n",
        "\n",
        "try:\n",
        "    os.mkdir(output_dir)\n",
        "except OSError as error:\n",
        "    print(error)\n",
        "\n",
        "status = recon_file_name == (original_dir + base_name + '.txt')\n",
        "print(status)\n",
        "\n",
        "try:\n",
        "    os.mkdir(model_dir + base_name)\n",
        "except OSError as error:\n",
        "    print(error)\n",
        "\n",
        "try:\n",
        "    os.mkdir(logs_dir + base_name)\n",
        "except OSError as error:\n",
        "    print(error)\n",
        "\n",
        "try:\n",
        "    os.mkdir(compressed_dir + base_name)\n",
        "except OSError as error:\n",
        "    print(error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OfLsthISMc_0"
      },
      "outputs": [],
      "source": [
        "# @title Trainer using a custom loop optimized for TPU\n",
        "#trainer\n",
        "# tf.random.set_seed(42)\n",
        "# np.random.seed(0)\n",
        "\n",
        "# train_iterator = iter(train_dataset)\n",
        "\n",
        "# with strategy.scope():\n",
        "#     model = getattr(Models, model_name)(batch_size, sequence_length, alphabet_size)\n",
        "#     optim = keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0, amsgrad=False)\n",
        "#     training_loss = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)\n",
        "\n",
        "#     def loss_object(y_true, y_pred):\n",
        "#         return 1/np.log(2) * K.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "# print(\"Starting training ...\")\n",
        "\n",
        "# @tf.function\n",
        "# def train_multiple_steps(iterator, steps):\n",
        "#     def step_fn(data):\n",
        "#         x, y = data\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             y_ = model(x, training=True)\n",
        "#             loss = loss_object(y, y_)\n",
        "#             loss_value = tf.nn.compute_average_loss(loss)\n",
        "\n",
        "#         gradients = tape.gradient(loss_value, model.trainable_variables)\n",
        "#         optim.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "#         training_loss.update_state(loss_value * strategy.num_replicas_in_sync)\n",
        "    \n",
        "#     for _ in tf.range(steps):\n",
        "#         strategy.run(step_fn, args=(next(iterator),))\n",
        "\n",
        "# train_multiple_steps(train_iterator, tf.convert_to_tensor(14))\n",
        "# print('Current step: {}, training loss: {}'.format(\n",
        "#       optim.iterations.numpy(),\n",
        "#       round(float(training_loss.result()), 4)))\n",
        "\n",
        "# #training loop\n",
        "# for epoch in range(num_epochs):\n",
        "#     with tqdm(train_dataset, unit=\" batches\") as tqdm_TD:\n",
        "#         for data in tqdm_TD:\n",
        "#             tqdm_TD.set_description(f\"Epoch {epoch+1}\")\n",
        "\n",
        "#             distributed_train_step(data, 64)\n",
        "#             evaluated_loss = str(training_loss.result().numpy())\n",
        "#             tqdm_TD.set_postfix(Loss = evaluated_loss)\n",
        "            \n",
        "#     model.save_weights(model_file)\n",
        "                                                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "sioGHx2AXiaU"
      },
      "outputs": [],
      "source": [
        "#@title Trainer using a custom loop\n",
        "#trainer\n",
        "\n",
        "# tf.random.set_seed(42)\n",
        "# np.random.seed(0)\n",
        "# from time import sleep\n",
        "\n",
        "# training_data = base_path + 'data/processed_files/' + base_name + '.npy'\n",
        "\n",
        "# print(\"Starting training ...\")\n",
        "\n",
        "# def loss_object(y_true, y_pred):\n",
        "#     return 1/np.log(2) * K.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "# def loss_fn(model, x, y, training):\n",
        "#     y_ = model(x, training=training)\n",
        "\n",
        "#     return loss_object(y_true=y, y_pred=y_)\n",
        "\n",
        "# def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S\n",
        "#     nrows = ((a.size - L) // S) + 1\n",
        "#     n = a.strides[0]\n",
        "#     return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n), writeable=False)\n",
        "\n",
        "# def generate_single_output_data(file_path,batch_size,time_steps):\n",
        "#     series = np.load(file_path)\n",
        "#     series = series.reshape(-1, 1)\n",
        "#     series = series.astype('uint8')\n",
        "#     print(series.itemsize, series.size)\n",
        "#     onehot_encoder = OneHotEncoder(sparse=False, dtype = bool)\n",
        "#     onehot_encoded = onehot_encoder.fit(series)\n",
        "#     series = series.reshape(-1)\n",
        "\n",
        "#     data = strided_app(series, time_steps+1, 1)\n",
        "#     l = int(len(data)/batch_size) * batch_size\n",
        "#     data = data[:l] \n",
        "    \n",
        "#     X = data[:, :-1]\n",
        "#     Y = data[:, -1:]\n",
        "    \n",
        "#     Y = onehot_encoder.transform(Y)\n",
        "#     alphabet_size = Y.shape[1]\n",
        "\n",
        "#     print(\"X itemsize : \", X.itemsize, \"X size : \", X.size, \"X shape : \", X.shape)\n",
        "#     print(\"X memory usage in GB : \", sys.getsizeof(X)/1000000000)\n",
        "#     print(\"Y itemsize : \", Y.itemsize, \"Y size : \", Y.size, \"Y shape : \", Y.shape)\n",
        "#     print(\"Y memory usage in GB : \", sys.getsizeof(Y)/1000000000)\n",
        "    \n",
        "#     train_dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
        "#     train_dataset = train_dataset.batch(batch_size)\n",
        "#     return train_dataset, alphabet_size\n",
        "\n",
        "# def grad(model, inputs, targets):\n",
        "#     with tf.GradientTape() as tape:\n",
        "#         loss_value = loss_fn(model, inputs, targets, training=True)\n",
        "#     return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
        "\n",
        "# def fit_model(train_dataset, nb_epoch, model):\n",
        "#     with strategy.scope():\n",
        "#         optim = keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0, amsgrad=False)\n",
        "#         model.compile(loss=loss_fn, optimizer=optim, steps_per_execution = 128)\n",
        "\n",
        "#     #training loop\n",
        "#     for epoch in range(nb_epoch):\n",
        "#         epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "#         with tqdm(train_dataset, unit=\" batches\") as tqdm_TD:\n",
        "#             for x,y in tqdm_TD:\n",
        "#                 tqdm_TD.set_description(f\"Epoch {epoch+1}\")\n",
        "\n",
        "#                 loss_value, grads = grad(model, x, y)\n",
        "#                 optim.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "#                 epoch_loss_avg.update_state(loss_value)\n",
        "#                 evaluated_loss = str(epoch_loss_avg.result().numpy())\n",
        "\n",
        "#                 tqdm_TD.set_postfix(Loss = evaluated_loss)\n",
        "                \n",
        "#         model.save_weights(model_file)\n",
        "                                                                \n",
        "\n",
        "# #batch_size=128\n",
        "# batch_size=1024\n",
        "# sequence_length=64\n",
        "# num_epochs=20\n",
        "\n",
        "# train_dataset, alphabet_size = generate_single_output_data(training_data, batch_size, sequence_length)\n",
        "\n",
        "# #print(train_dataset.element_spec)\n",
        "\n",
        "# with strategy.scope():\n",
        "#     model = getattr(Models, model_name)(batch_size, sequence_length, alphabet_size)\n",
        "\n",
        "# print('Fitting model ...')\n",
        "# fit_model(train_dataset, num_epochs, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N5H1VHVpXvLc"
      },
      "outputs": [],
      "source": [
        "# @title Trainer with tf.data.Dataset - big files - quarters\n",
        "\n",
        "# # trainer\n",
        "\n",
        "# tf.random.set_seed(42)\n",
        "# np.random.seed(0)\n",
        "\n",
        "# training_data = base_path + 'data/processed_files/' + base_name + '.npy'\n",
        "\n",
        "# print(\"Starting training ...\")\n",
        "\n",
        "# def loss_fn(y_true, y_pred):\n",
        "#     #print(y_true.shape, y_pred.shape)\n",
        "#     return 1/np.log(2) * K.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "# def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S\n",
        "#     nrows = ((a.size - L) // S) + 1\n",
        "#     n = a.strides[0]\n",
        "#     return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n), writeable=False)\n",
        "\n",
        "# def generate_single_output_data(file_path,batch_size,time_steps):\n",
        "#     series = np.load(file_path)\n",
        "#     series = series.reshape(-1, 1)\n",
        "#     series = series.astype('uint8')\n",
        "#     print(series.itemsize, series.size)\n",
        "#     onehot_encoder = OneHotEncoder(sparse=False, dtype = 'uint8')\n",
        "#     onehot_encoded = onehot_encoder.fit(series)\n",
        "#     series = series.reshape(-1)\n",
        "\n",
        "#     data = strided_app(series, time_steps+1, 1)\n",
        "#     l = int(len(data)/batch_size) * batch_size\n",
        "#     data = data[:l] \n",
        "    \n",
        "#     X = data[:, :-1]\n",
        "#     Y = data[:, -1:]\n",
        "    \n",
        "#     Y = onehot_encoder.transform(Y)\n",
        "\n",
        "#     Y = np.matmul(Y,conversion_matrix)\n",
        "#     # Y = Y.astype('uint8')\n",
        "\n",
        "#     alphabet_size = Y.shape[1]\n",
        "\n",
        "#     print(\"X itemsize : \", X.itemsize, \"X size : \", X.size, \"X shape : \", X.shape)\n",
        "#     print(\"X memory usage in GB : \", sys.getsizeof(X)/1000000000)\n",
        "#     print(\"Y itemsize : \", Y.itemsize, \"Y size : \", Y.size, \"Y shape : \", Y.shape)\n",
        "#     print(\"Y memory usage in GB : \", sys.getsizeof(Y)/1000000000)\n",
        "\n",
        "#     quarter = int(len(X)/4)\n",
        "#     x1 = X[:quarter]\n",
        "#     y1 = Y[:quarter]\n",
        "#     x2 = X[quarter:2*quarter]\n",
        "#     y2 = Y[quarter:2*quarter]\n",
        "#     x3 = X[2*quarter:3*quarter]\n",
        "#     y3 = Y[2*quarter:3*quarter]\n",
        "#     x4 = X[3*quarter:]\n",
        "#     y4 = Y[3*quarter:]\n",
        "\n",
        "#     train_dataset_1 = tf.data.Dataset.from_tensor_slices((x1, y1))\n",
        "#     print(\"Created dataset 1\")\n",
        "#     train_dataset_1 = train_dataset_1.batch(int(quarter/2))\n",
        "#     print(\"Split dataset 1 into batches\")\n",
        "\n",
        "#     train_dataset_2 = tf.data.Dataset.from_tensor_slices((x2, y2))\n",
        "#     print(\"Created dataset 2\")\n",
        "#     train_dataset_2 = train_dataset_2.batch(int(quarter/2))\n",
        "#     print(\"Split dataset 2 into batches\")\n",
        "\n",
        "#     train_dataset_3 = tf.data.Dataset.from_tensor_slices((x3, y3))\n",
        "#     print(\"Created dataset 3\")\n",
        "#     train_dataset_3 = train_dataset_3.batch(int(quarter/2))\n",
        "#     print(\"Split dataset 3 into batches\")\n",
        "\n",
        "#     train_dataset_4 = tf.data.Dataset.from_tensor_slices((x2, y2))\n",
        "#     print(\"Created dataset 4\")\n",
        "#     train_dataset_4 = train_dataset_4.batch(int(quarter/2))\n",
        "#     print(\"Split dataset 4 into batches\")\n",
        "\n",
        "#     return train_dataset_1, train_dataset_2, train_dataset_3, train_dataset_4, alphabet_size\n",
        "    \n",
        "\n",
        "# def fit_model(train_1, train_2, train_3, train_4, bs, nb_epoch, model):\n",
        "#     with strategy.scope():\n",
        "#         optim = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0, amsgrad=False)\n",
        "#         model.compile(loss=loss_fn, optimizer=optim, steps_per_execution = 128)\n",
        "\n",
        "#     checkpoint = ModelCheckpoint(model_file, monitor='loss', verbose=1, save_best_only=False, mode='min', save_weights_only=True)\n",
        "#     csv_logger = CSVLogger(log_file, append=True, separator=';')\n",
        "#     #early_stopping = EarlyStopping(monitor='loss', mode='min', min_delta=0.005, patience=3, verbose=1)\n",
        "#     callbacks_list = [checkpoint, csv_logger]\n",
        "#     for i in range(nb_epoch):\n",
        "#         print(\"Epoch: \", i)\n",
        "#         train_1 = train_1.shuffle(buffer_size = len(train_1))\n",
        "#         train_2 = train_2.shuffle(buffer_size = len(train_2))\n",
        "#         train_3 = train_3.shuffle(buffer_size = len(train_3))\n",
        "#         train_4 = train_4.shuffle(buffer_size = len(train_4))\n",
        "\n",
        "#         j = 1\n",
        "#         for x,y in train_1:\n",
        "#             print(\"Minibatch number\", j)\n",
        "#             model.fit(x, y, epochs=1, batch_size=bs, verbose=1, callbacks=callbacks_list)\n",
        "#             j = j + 1\n",
        "        \n",
        "#         for x,y in train_2:\n",
        "#             print(\"Minibatch number\", j)\n",
        "#             model.fit(x, y, epochs=1, batch_size=bs, verbose=1, callbacks=callbacks_list)\n",
        "#             j = j + 1\n",
        "\n",
        "#         for x,y in train_3:\n",
        "#             print(\"Minibatch number\", j)\n",
        "#             model.fit(x, y, epochs=1, batch_size=bs, verbose=1, callbacks=callbacks_list)\n",
        "#             j = j + 1\n",
        "\n",
        "#         for x,y in train_4:\n",
        "#             print(\"Minibatch number\", j)\n",
        "#             model.fit(x, y, epochs=1, batch_size=bs, verbose=1, callbacks=callbacks_list)\n",
        "#             j = j + 1\n",
        "        \n",
        "# #batch_size=128\n",
        "# batch_size=1024\n",
        "# sequence_length=64\n",
        "# num_epochs=5\n",
        "\n",
        "# training_data_1, training_data_2, training_data_3, training_data_4, alphabet_size = generate_single_output_data(training_data, batch_size, sequence_length)\n",
        "\n",
        "# with strategy.scope():\n",
        "#     model = getattr(Models, model_name)(batch_size, sequence_length, alphabet_size)\n",
        "# print('Fitting model ...')\n",
        "# fit_model(training_data_1, training_data_2, training_data_3, training_data_4, batch_size, num_epochs, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6C31DkGkg-o",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Trainer with tf.data.Dataset - halves\n",
        "\n",
        "#trainer\n",
        "\n",
        "# tf.random.set_seed(42)\n",
        "# np.random.seed(0)\n",
        "\n",
        "# training_data = base_path + 'data/processed_files/' + base_name + '.npy'\n",
        "\n",
        "# print(\"Starting training ...\")\n",
        "\n",
        "# def loss_fn(y_true, y_pred):\n",
        "#     #print(y_true.shape, y_pred.shape)\n",
        "#     return 1/np.log(2) * K.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "# def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S\n",
        "#     nrows = ((a.size - L) // S) + 1\n",
        "#     n = a.strides[0]\n",
        "#     return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n), writeable=False)\n",
        "\n",
        "# def generate_single_output_data(file_path,batch_size,time_steps):\n",
        "#     series = np.load(file_path)\n",
        "#     series = series.reshape(-1, 1)\n",
        "#     series = series.astype('uint8')\n",
        "#     print(series.itemsize, series.size)\n",
        "#     onehot_encoder = OneHotEncoder(sparse=False, dtype = 'uint8')\n",
        "#     onehot_encoded = onehot_encoder.fit(series)\n",
        "#     series = series.reshape(-1)\n",
        "\n",
        "#     data = strided_app(series, time_steps+1, 1)\n",
        "#     l = int(len(data)/batch_size) * batch_size\n",
        "#     data = data[:l] \n",
        "    \n",
        "#     X = data[:, :-1]\n",
        "#     Y = data[:, -1:]\n",
        "    \n",
        "#     Y = onehot_encoder.transform(Y)\n",
        "    \n",
        "#     Y = np.matmul(Y,conversion_matrix)\n",
        "#     Y = Y.astype('uint8')\n",
        "\n",
        "#     alphabet_size = Y.shape[1]\n",
        "\n",
        "#     print(\"X itemsize : \", X.itemsize, \"X size : \", X.size, \"X shape : \", X.shape)\n",
        "#     print(\"X memory usage in GB : \", sys.getsizeof(X)/1000000000)\n",
        "#     print(\"Y itemsize : \", Y.itemsize, \"Y size : \", Y.size, \"Y shape : \", Y.shape)\n",
        "#     print(\"Y memory usage in GB : \", sys.getsizeof(Y)/1000000000)\n",
        "\n",
        "#     half = int(len(X)/2)\n",
        "#     x1 = X[:half]\n",
        "#     y1 = Y[:half]\n",
        "#     x2 = X[half:]\n",
        "#     y2 = Y[half:]\n",
        "\n",
        "#     train_dataset_1 = tf.data.Dataset.from_tensor_slices((x1, y1))\n",
        "#     print(\"Created dataset 1\")\n",
        "#     train_dataset_1 = train_dataset_1.batch(int(half/2))\n",
        "#     print(\"Split dataset 1 into batches\")\n",
        "\n",
        "#     train_dataset_2 = tf.data.Dataset.from_tensor_slices((x2, y2))\n",
        "#     print(\"Created dataset 2\")\n",
        "#     train_dataset_2 = train_dataset_2.batch(int(half/2))\n",
        "#     print(\"Split dataset 2 into batches\")\n",
        "\n",
        "#     return train_dataset_1, train_dataset_2, alphabet_size\n",
        "    \n",
        "\n",
        "# def fit_model(train_1, train_2, bs, nb_epoch, model):\n",
        "#     with strategy.scope():\n",
        "#         optim = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0, amsgrad=False)\n",
        "#         model.compile(loss=loss_fn, optimizer=optim, steps_per_execution = 128)\n",
        "\n",
        "#     checkpoint = ModelCheckpoint(model_file, monitor='loss', verbose=1, save_best_only=False, mode='min', save_weights_only=True)\n",
        "#     csv_logger = CSVLogger(log_file, append=True, separator=';')\n",
        "#     early_stopping = EarlyStopping(monitor='loss', mode='min', min_delta=0.005, patience=3, verbose=1)\n",
        "#     callbacks_list = [checkpoint, csv_logger]\n",
        "#     for i in range(nb_epoch):\n",
        "#         train_1 = train_1.shuffle(buffer_size = len(train_1))\n",
        "#         train_2 = train_2.shuffle(buffer_size = len(train_2))\n",
        "\n",
        "#         j = 1\n",
        "#         for x,y in train_1:\n",
        "#             print(\"Minibatch number\", j)\n",
        "#             model.fit(x, y, epochs=i+2, initial_epoch=i, batch_size=bs, verbose=1, callbacks=callbacks_list)\n",
        "#             j = j + 1\n",
        "        \n",
        "#         for x,y in train_2:\n",
        "#             print(\"Minibatch number\", j)\n",
        "#             model.fit(x, y, epochs=i+2, initial_epoch=i, batch_size=bs, verbose=1, callbacks=callbacks_list)\n",
        "#             j = j + 1\n",
        "        \n",
        "# #batch_size=128\n",
        "# batch_size=1024\n",
        "# sequence_length=64\n",
        "# num_epochs=5\n",
        "\n",
        "# training_data_1, training_data_2, alphabet_size = generate_single_output_data(training_data, batch_size, sequence_length)\n",
        "\n",
        "# with strategy.scope():\n",
        "#     model = getattr(Models, model_name)(batch_size, sequence_length, alphabet_size)\n",
        "# print('Fitting model ...')\n",
        "# fit_model(training_data_1, training_data_2, batch_size, num_epochs, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzJldsA1lrWY",
        "outputId": "92c4c724-e3fb-42da-dc65-e1579d80ce24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training ...\n",
            "1 465559\n",
            "X itemsize :  1 X size :  29753344 X shape :  (464896, 64)\n",
            "X memory usage in GB :  1.2e-07\n",
            "Y itemsize :  1 Y size :  59506688 Y shape :  (464896, 128)\n",
            "Y memory usage in GB :  0.059506808\n",
            "Fitting model ...\n",
            "Epoch 1/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 4.3541\n",
            "Epoch 1: loss improved from inf to 4.35409, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 47s 104ms/step - loss: 4.3541\n",
            "Epoch 2/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 3.5351\n",
            "Epoch 2: loss improved from 4.35409 to 3.53508, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 3.5351\n",
            "Epoch 3/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 3.2934\n",
            "Epoch 3: loss improved from 3.53508 to 3.29340, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 3.2934\n",
            "Epoch 4/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 3.2420\n",
            "Epoch 4: loss improved from 3.29340 to 3.24202, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 3.2420\n",
            "Epoch 5/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 3.0586\n",
            "Epoch 5: loss improved from 3.24202 to 3.05855, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 3.0586\n",
            "Epoch 6/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.9941\n",
            "Epoch 6: loss improved from 3.05855 to 2.99407, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 2.9941\n",
            "Epoch 7/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.8879\n",
            "Epoch 7: loss improved from 2.99407 to 2.88792, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 2.8879\n",
            "Epoch 8/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.8400\n",
            "Epoch 8: loss improved from 2.88792 to 2.84001, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 2.8400\n",
            "Epoch 9/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.8112\n",
            "Epoch 9: loss improved from 2.84001 to 2.81118, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 2.8112\n",
            "Epoch 10/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.8103\n",
            "Epoch 10: loss improved from 2.81118 to 2.81033, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 2.8103\n",
            "Epoch 11/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.7707\n",
            "Epoch 11: loss improved from 2.81033 to 2.77069, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 2.7707\n",
            "Epoch 12/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.7400\n",
            "Epoch 12: loss improved from 2.77069 to 2.74001, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 2.7400\n",
            "Epoch 13/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.6939\n",
            "Epoch 13: loss improved from 2.74001 to 2.69385, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 2.6939\n",
            "Epoch 14/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.7092\n",
            "Epoch 14: loss did not improve from 2.69385\n",
            "454/454 [==============================] - 28s 62ms/step - loss: 2.7092\n",
            "Epoch 15/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.6785\n",
            "Epoch 15: loss improved from 2.69385 to 2.67848, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 65ms/step - loss: 2.6785\n",
            "Epoch 16/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.6945\n",
            "Epoch 16: loss did not improve from 2.67848\n",
            "454/454 [==============================] - 28s 62ms/step - loss: 2.6945\n",
            "Epoch 17/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.6463\n",
            "Epoch 17: loss improved from 2.67848 to 2.64625, saving model to /content/drive/MyDrive/DeepZip/data/trained_models/HP1_clean/TransformerLarge_128.hdf5\n",
            "454/454 [==============================] - 29s 64ms/step - loss: 2.6463\n",
            "Epoch 18/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.6551\n",
            "Epoch 18: loss did not improve from 2.64625\n",
            "454/454 [==============================] - 28s 62ms/step - loss: 2.6551\n",
            "Epoch 19/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.6714\n",
            "Epoch 19: loss did not improve from 2.64625\n",
            "454/454 [==============================] - 28s 62ms/step - loss: 2.6714\n",
            "Epoch 20/20\n",
            "454/454 [==============================] - ETA: 0s - loss: 2.6575\n",
            "Epoch 20: loss did not improve from 2.64625\n",
            "454/454 [==============================] - 28s 62ms/step - loss: 2.6575\n",
            "Epoch 20: early stopping\n"
          ]
        }
      ],
      "source": [
        "#@title Trainer without data.Dataset\n",
        "#trainer\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(0)\n",
        "\n",
        "training_data = base_path + 'data/processed_files/' + base_name + '.npy'\n",
        "\n",
        "print(\"Starting training ...\")\n",
        "\n",
        "def loss_fn(y_true, y_pred):\n",
        "    #print(y_true.shape, y_pred.shape)\n",
        "    return 1/np.log(2) * K.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "def strided_app(a, L, S):  # Window len = L, Stride len/stepsize = S\n",
        "    nrows = ((a.size - L) // S) + 1\n",
        "    n = a.strides[0]\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n), writeable=False)\n",
        "\n",
        "def generate_single_output_data(file_path,batch_size,time_steps):\n",
        "    series = np.load(file_path)\n",
        "    series = series.reshape(-1, 1)\n",
        "    series = series.astype('uint8')\n",
        "    print(series.itemsize, series.size)\n",
        "    onehot_encoder = OneHotEncoder(sparse=False, dtype = 'uint8')\n",
        "    onehot_encoded = onehot_encoder.fit(series)\n",
        "    series = series.reshape(-1)\n",
        "\n",
        "    data = strided_app(series, time_steps+1, 1)\n",
        "    l = int(len(data)/batch_size) * batch_size\n",
        "    data = data[:l] \n",
        "    \n",
        "    X = data[:, :-1]\n",
        "    Y = data[:, -1:]\n",
        "    \n",
        "    Y = onehot_encoder.transform(Y)\n",
        "\n",
        "    Y = np.matmul(Y,conversion_matrix)\n",
        "    Y = Y.astype('uint8')\n",
        "    \n",
        "    alphabet_size = Y.shape[1]\n",
        "\n",
        "    print(\"X itemsize : \", X.itemsize, \"X size : \", X.size, \"X shape : \", X.shape)\n",
        "    print(\"X memory usage in GB : \", sys.getsizeof(X)/1000000000)\n",
        "    print(\"Y itemsize : \", Y.itemsize, \"Y size : \", Y.size, \"Y shape : \", Y.shape)\n",
        "    print(\"Y memory usage in GB : \", sys.getsizeof(Y)/1000000000)\n",
        "\n",
        "    return X, Y, alphabet_size\n",
        "    \n",
        "\n",
        "def fit_model(x, y, bs, nb_epoch, model):\n",
        "    with strategy.scope():\n",
        "        optim = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0, amsgrad=False)\n",
        "        model.compile(loss=loss_fn, optimizer=optim, steps_per_execution = 128)\n",
        "\n",
        "    checkpoint = ModelCheckpoint(model_file, monitor='loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)\n",
        "    csv_logger = CSVLogger(log_file, append=True, separator=';')\n",
        "    early_stopping = EarlyStopping(monitor='loss', mode='min', min_delta=0.005, patience=3, verbose=1)\n",
        "    callbacks_list = [checkpoint, csv_logger, early_stopping]\n",
        "    \n",
        "    model.fit(x, y, epochs=nb_epoch, shuffle=True, batch_size=bs, verbose=1, callbacks=callbacks_list)\n",
        "\n",
        "batch_size=1024\n",
        "sequence_length=64\n",
        "num_epochs=20\n",
        "\n",
        "X, Y, alphabet_size = generate_single_output_data(training_data, batch_size, sequence_length)\n",
        "\n",
        "with strategy.scope():\n",
        "    model = getattr(Models, model_name)(batch_size, sequence_length, alphabet_size)\n",
        "print('Fitting model ...')\n",
        "fit_model(X, Y, batch_size, num_epochs, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8q8MyRa0IAH"
      },
      "outputs": [],
      "source": [
        "#@title Compressor\n",
        "\n",
        "print('Starting Compression ...')\n",
        "\n",
        "def strided_app(a, L, S):\n",
        "    nrows = ((a.size - L) // S) + 1\n",
        "    n = a.strides[0]\n",
        "    return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n), writeable=False)\n",
        "\n",
        "def predict_lstm(X, y, y_original, timesteps, bs, alphabet_size, model_name, final_step=False):\n",
        "    with strategy.scope():\n",
        "        model = getattr(Models, model_name)(bs, timesteps, alphabet_size)\n",
        "    \n",
        "    model(np.zeros(shape=(128,64), dtype = np.uint8))\n",
        "    model.load_weights(model_file)\n",
        "\n",
        "    if not final_step:\n",
        "        num_iters = int((len(X)+timesteps)/bs)\n",
        "        ind = np.array(range(bs))*num_iters\n",
        "        \n",
        "        f = [open(temp_file_prefix + '.' + str(i),'wb') for i in range(bs)]\n",
        "        bitout = [BitOutputStream(f[i]) for i in range(bs)]\n",
        "        enc = [ArithmeticEncoder(32, bitout[i]) for i in range(bs)]\n",
        "        prob = np.ones(alphabet_size)/alphabet_size\n",
        "        cumul = np.zeros(alphabet_size + 1, dtype = np.uint64)\n",
        "        cumul[1:] = np.cumsum(prob*10000000 + 1)    \n",
        "\n",
        "        for i in range(bs):\n",
        "            for j in range(min(timesteps, num_iters)):\n",
        "                enc[i].write(cumul, X[ind[i],j])\n",
        "\n",
        "        cumul = np.zeros((bs, alphabet_size+1), dtype = np.uint64)\n",
        "        \n",
        "        for j in tqdm(range(num_iters - timesteps)):\n",
        "            prob = model.predict(X[ind,:], batch_size=bs)\n",
        "            cumul[:,1:] = np.cumsum(prob*10000000 + 1, axis = 1)\n",
        "\n",
        "            for i in range(bs):\n",
        "                enc[i].write(cumul[i,:], y_original[ind[i]])\n",
        "\n",
        "            ind = ind + 1\n",
        "\n",
        "        # close files\n",
        "        for i in range(bs):\n",
        "            enc[i].finish()\n",
        "            bitout[i].close()\n",
        "            f[i].close()            \n",
        "    else:\n",
        "        f = open(temp_file_prefix + '.last', 'wb')\n",
        "        bitout = BitOutputStream(f)\n",
        "        enc = ArithmeticEncoder(32, bitout)\n",
        "        prob = np.ones(alphabet_size)/alphabet_size\n",
        "        cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n",
        "        cumul[1:] = np.cumsum(prob*10000000 + 1)        \n",
        "\n",
        "        for j in range(timesteps):\n",
        "            enc.write(cumul, X[0,j])\n",
        "\n",
        "        for i in tqdm(range(len(X))):\n",
        "            prob = model.predict(X[i,:].reshape(1,-1), batch_size=1)\n",
        "            cumul[1:] = np.cumsum(prob*10000000 + 1)\n",
        "            enc.write(cumul, y_original[i][0])\n",
        "\n",
        "        enc.finish()\n",
        "        bitout.close()\n",
        "        f.close()\n",
        "    return\n",
        "\n",
        "def var_int_encode(byte_str_len, f):\n",
        "    while True:\n",
        "        this_byte = byte_str_len&127\n",
        "        byte_str_len >>= 7\n",
        "        if byte_str_len == 0:\n",
        "            f.write(struct.pack('B',this_byte))\n",
        "            break\n",
        "\n",
        "        f.write(struct.pack('B',this_byte|128))\n",
        "        byte_str_len -= 1\n",
        "\n",
        "#main\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "temp_file_prefix = temp_dir + \"/compressed\"\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(0)\n",
        "series = np.load(training_data)\n",
        "series = series.reshape(-1, 1)\n",
        "onehot_encoder = OneHotEncoder(sparse=False, dtype = bool)\n",
        "onehot_encoded = onehot_encoder.fit(series)\n",
        "\n",
        "print(len(series))\n",
        "batch_size = 1024\n",
        "\n",
        "timesteps = 64\n",
        "\n",
        "\n",
        "with open(params_file, 'r') as f:\n",
        "    params = json.load(f)\n",
        "\n",
        "params['len_series'] = len(series)\n",
        "params['bs'] = batch_size\n",
        "params['timesteps'] = timesteps\n",
        "\n",
        "with open(output_prefix + '.params', 'w') as f:\n",
        "    json.dump(params, f, indent=4)\n",
        "\n",
        "alphabet_size = len(params['id2char_dict'])\n",
        "\n",
        "series = series.reshape(-1)\n",
        "data = strided_app(series, timesteps+1, 1)\n",
        "\n",
        "X = data[:, :-1]\n",
        "Y_original = data[:, -1:]\n",
        "\n",
        "X = X.astype('uint8')\n",
        "Y_original = Y_original.astype('uint8')\n",
        "Y = onehot_encoder.transform(Y_original)\n",
        "\n",
        "print(\"X itemsize : \", X.itemsize, \"X size : \", X.size, \"X shape : \", X.shape)\n",
        "print(\"X memory usage in GB : \", sys.getsizeof(X)/1000000000)\n",
        "print(\"Y itemsize : \", Y.itemsize, \"Y size : \", Y.size, \"Y shape : \", Y.shape)\n",
        "print(\"Y memory usage in GB : \", sys.getsizeof(Y)/1000000000)\n",
        "\n",
        "l = int(len(series)/batch_size)*batch_size\n",
        "\n",
        "with strategy.scope():\n",
        "    predict_lstm(X, Y, Y_original, timesteps, batch_size, alphabet_size, model_name)\n",
        "\n",
        "if l < len(series)-timesteps:\n",
        "    with strategy.scope():\n",
        "        predict_lstm(X[l:,:], Y[l:,:], Y_original[l:], timesteps, 1, alphabet_size, model_name, final_step = True)\n",
        "else:\n",
        "    f = open(temp_file_prefix + '.last', 'wb')\n",
        "    bitout = BitOutputStream(f)\n",
        "    enc = ArithmeticEncoder(32, bitout) \n",
        "    prob = np.ones(alphabet_size)/alphabet_size\n",
        "    \n",
        "    cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n",
        "    cumul[1:] = np.cumsum(prob*10000000 + 1)        \n",
        "    for j in range(l, len(series)):\n",
        "        enc.write(cumul, series[j])\n",
        "\n",
        "    enc.finish()\n",
        "    bitout.close() \n",
        "    f.close()\n",
        "\n",
        "f = open(output_prefix + '.combined', 'wb')\n",
        "for i in range(batch_size):\n",
        "    f_in = open(temp_file_prefix + '.' + str(i), 'rb')\n",
        "    byte_str = f_in.read()\n",
        "    byte_str_len = len(byte_str)\n",
        "    var_int_encode(byte_str_len, f)\n",
        "    f.write(byte_str)\n",
        "    f_in.close()\n",
        "\n",
        "f_in = open(temp_file_prefix + '.last', 'rb')\n",
        "byte_str = f_in.read()\n",
        "byte_str_len = len(byte_str)\n",
        "var_int_encode(byte_str_len, f)\n",
        "f.write(byte_str)\n",
        "f_in.close()\n",
        "f.close()\n",
        "shutil.rmtree(temp_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "uXaeNhPM1U3P"
      },
      "outputs": [],
      "source": [
        "#@title Decompressor\n",
        "\n",
        "# input_file_prefix = output_dir + '/' + model_name + '.compressed'\n",
        "# print(input_file_prefix)\n",
        "\n",
        "# def strided_app(a, L, S):\n",
        "#     nrows = ((a.size - L) // S) + 1\n",
        "#     n = a.strides[0]\n",
        "#     return np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n), writeable=False)\n",
        "\n",
        "# def create_data(rows, p=0.5):\n",
        "#     data = np.random.choice(2, rows, p=[p, 1-p])\n",
        "#     print(np.sum(data))/np.float32(len(data))\n",
        "#     return data\n",
        " \n",
        "# def predict_lstm(len_series, timesteps, bs, alphabet_size, model_name, final_step=False):\n",
        "#     with strategy.scope():\n",
        "#         model = getattr(Models, model_name)(bs, timesteps, alphabet_size)\n",
        "#     model(np.zeros(shape=(128,64), dtype = np.uint8))\n",
        "#     model.load_weights(model_file)\n",
        "    \n",
        "#     if not final_step:\n",
        "#         num_iters = int((len_series)/bs)\n",
        "#         series_2d = np.zeros((bs,num_iters), dtype = np.uint8)\n",
        "#         f = [open(temp_file_prefix+'.'+str(i),'rb') for i in range(bs)]\n",
        "#         bitin = [BitInputStream(f[i]) for i in range(bs)]\n",
        "#         dec = [ArithmeticDecoder(32, bitin[i]) for i in range(bs)]\n",
        "#         prob = np.ones(alphabet_size)/alphabet_size\n",
        "#         cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n",
        "#         cumul[1:] = np.cumsum(prob*10000000 + 1)              \n",
        "\n",
        "#         for i in range(bs):\n",
        "#             for j in range(min(num_iters,timesteps)):\n",
        "#                 series_2d[i,j] = dec[i].read(cumul, alphabet_size)\n",
        "\n",
        "#         cumul = np.zeros((bs, alphabet_size+1), dtype = np.uint64)\n",
        "\n",
        "#         for j in tqdm(range(num_iters - timesteps)):\n",
        "#             prob = model.predict(series_2d[:,j:j+timesteps], batch_size=bs)\n",
        "#             cumul[:,1:] = np.cumsum(prob*10000000 + 1, axis = 1)\n",
        "#             for i in range(bs):\n",
        "#                 series_2d[i,j+timesteps] = dec[i].read(cumul[i,:], alphabet_size)\n",
        "\n",
        "#         for i in range(bs):\n",
        "#             bitin[i].close()\n",
        "#             f[i].close()\n",
        "\n",
        "#         return series_2d.reshape(-1)\n",
        "\n",
        "#     else:\n",
        "#         series = np.zeros(len_series, dtype = np.uint8)\n",
        "#         f = open(temp_file_prefix + '.last', 'rb')\n",
        "#         bitin = BitInputStream(f)\n",
        "#         dec = ArithmeticDecoder(32, bitin)\n",
        "#         prob = np.ones(alphabet_size)/alphabet_size\n",
        "\n",
        "#         cumul = np.zeros(alphabet_size+1, dtype = np.uint64)\n",
        "#         cumul[1:] = np.cumsum(prob*10000000 + 1)                \n",
        "#         for j in range(min(timesteps,len_series)):\n",
        "#             series[j] = dec.read(cumul, alphabet_size)\n",
        "            \n",
        "#         for i in tqdm(range(len_series-timesteps)):\n",
        "#             prob = model.predict(series[i:i+timesteps].reshape(1,-1), batch_size=1)\n",
        "#             cumul[1:] = np.cumsum(prob*10000000 + 1)\n",
        "#             series[i+timesteps] = dec.read(cumul, alphabet_size)\n",
        "#         bitin.close()\n",
        "#         f.close()\n",
        "#         return series\n",
        "\n",
        "# def arithmetic_step(prob, freqs, dec):\n",
        "#     freqs.update_table(prob*10000000+1)\n",
        "#     return dec.read(freqs)\n",
        "\n",
        "# def var_int_decode(f):\n",
        "#     byte_str_len = 0\n",
        "#     shift = 1\n",
        "#     while True:\n",
        "#         this_byte = struct.unpack('B', f.read(1))[0]\n",
        "#         byte_str_len += (this_byte & 127) * shift\n",
        "#         if this_byte & 128 == 0:\n",
        "#             break\n",
        "#         shift <<= 7\n",
        "#         byte_str_len += shift\n",
        "#     return byte_str_len\n",
        "\n",
        "# #main\n",
        "# temp_dir = tempfile.mkdtemp()\n",
        "# temp_file_prefix = temp_dir + \"/compressed\"\n",
        "\n",
        "# tf.random.set_seed(42)\n",
        "# np.random.seed(0)\n",
        "\n",
        "# f = open(input_file_prefix + '.params','r')\n",
        "# param_dict = json.loads(f.read())\n",
        "# f.close()\n",
        "# len_series = param_dict['len_series']\n",
        "# batch_size = param_dict['bs']\n",
        "# timesteps = param_dict['timesteps']\n",
        "# id2char_dict = param_dict['id2char_dict']\n",
        "\n",
        "# f = open(input_file_prefix+'.combined','rb')\n",
        "\n",
        "# for i in range(batch_size):\n",
        "#     f_out = open(temp_file_prefix+'.'+str(i),'wb')\n",
        "#     byte_str_len = var_int_decode(f)\n",
        "#     byte_str = f.read(byte_str_len)\n",
        "#     f_out.write(byte_str)\n",
        "#     f_out.close()\n",
        "\n",
        "# f_out = open(temp_file_prefix+'.last','wb')\n",
        "# byte_str_len = var_int_decode(f)\n",
        "# byte_str = f.read(byte_str_len)\n",
        "# f_out.write(byte_str)\n",
        "# f_out.close()\n",
        "# f.close()\n",
        "\n",
        "# series = np.zeros(len_series,dtype=np.uint8)\n",
        "\n",
        "# l = int(len_series/batch_size)*batch_size\n",
        "# alphabet_size = len(id2char_dict)\n",
        "# with strategy.scope():\n",
        "#     series[:l] = predict_lstm(l, timesteps, batch_size, alphabet_size, model_name)\n",
        "\n",
        "# if l < len_series:\n",
        "#     with strategy.scope():\n",
        "#         series[l:] = predict_lstm(len_series - l, timesteps, 1, alphabet_size, model_name, final_step = True)\n",
        "\n",
        "# f = open(recon_file_name,'wb')\n",
        "# print(id2char_dict)\n",
        "# print(series[:10])\n",
        "# f.write(bytearray([id2char_dict[str(s)] for s in series]))\n",
        "# f.close()\n",
        "# shutil.rmtree(temp_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZlNhg6VPQW5"
      },
      "outputs": [],
      "source": [
        "#@title Results\n",
        "import filecmp\n",
        "\n",
        "compressed_file_path = input_file_prefix + '.combined'\n",
        "\n",
        "model_size = os.stat(model_file).st_size\n",
        "compressed_size = os.stat(compressed_file_path).st_size + model_size\n",
        "original_size = os.stat(input_file_path).st_size\n",
        "# reconstructed_size = os.stat(recon_file_name).st_size\n",
        "\n",
        "print(f'Original file size is {original_size} bytes')\n",
        "print(f'Compressed file size is {compressed_size} bytes')\n",
        "# print(f'Reconstructed file size is {reconstructed_size} bytes')\n",
        "print(f'Model size is {model_size} bytes')\n",
        "\n",
        "# diff = filecmp.cmp(input_file_path, recon_file_name)\n",
        "# if diff:\n",
        "#     print(\"Reconstructed file is identical to original file\")\n",
        "# else:\n",
        "#     print(\"Reconstructed file is not identical to original file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brISjaGcEYTX"
      },
      "outputs": [],
      "source": [
        "def model_size(m, x, a):\n",
        "    return 8*(m*(9*x*x + 13*x) + a*x + a)\n",
        "\n",
        "m = 1\n",
        "a = 200\n",
        "x = 1024\n",
        "\n",
        "print(model_size(m,x,a)/1000000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Colab_DeepZip.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}